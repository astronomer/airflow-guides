---
title: "Dynamic Tasks in Airflow"
description: "How to dynamically create tasks at runtime in your Airflow DAGs."
date: 2022-04-28T00:00:00.000Z
slug: "dynamic-tasks"
heroImagePath: null
tags: ["Tasks"]
---

With the release of [Airflow 2.3](https://airflow.apache.org/blog/airflow-2.3.0/), users can write DAGs that dynamically generate parallel tasks at runtime. This feature, known as dynamic task mapping, is a paradigm shift for DAG design in Airflow.

Prior to Airflow 2.3, tasks could only be generated dynamically at the time that the DAG was parsed, meaning you had to change your DAG code if you needed to adjust tasks based on some external factor. With dynamic task mapping, you can easily write DAGs that create tasks based on your current runtime environment.

In this guide, we'll explain the concept of dynamic task mapping and provide example implementations for common use cases.

## Assumed knowledge

To get the most out of this guide, you should have knowledge of:

- Airflow Operators. See [Operators 101](https://www.astronomer.io/guides/what-is-an-operator/).
- How to use Airflow decorators to define tasks. See [Introduction to Airflow Decorators](https://www.astronomer.io/guides/airflow-decorators/).
- XComs in Airflow. See [Passing Data Between Airflow Tasks](https://www.astronomer.io/guides/airflow-passing-data-between-tasks/).

## Dynamic task concepts

Airflow's dynamic task mapping feature is built off of the [MapReduce](https://en.wikipedia.org/wiki/MapReduce) programming model. The map procedure takes a set of inputs and creates a single task for each one. The reduce procedure, which is optional, allows a task to operate on the collected output of a mapped task. In practice, this means that your DAG can create an arbitrary number of parallel tasks at runtime based on some input parameter(s) (the "map"), and then if needed, have a single task downstream of your parallel mapped tasks that depends on their output (the "reduce").

Airflow tasks have two new functions available to implement the "map" portion of dynamic task mapping. For the task you want to map, all operator parameters must be passed through one of these two functions.

- `expand()`: This function passes the parameter or parameters that you want to map on. A separate parallel task will be created for each input.
- `partial()`: This function passes any parameters that remain constant across all mapped tasks which are generated by `expand()`.

> **Note**: In Airflow 2.4 the possibility to map over several sets of keyword arguments was added. This type of mapping uses the function `expand_kwargs()` instead of `expand()` and is explained in the section 'Mapping over multiple parameters' below.

For example, the following task uses both of these functions to dynamically generate 3 task runs:

```python
@task
def add(x: int, y: int):
    return x + y

added_values = add.partial(y=10).expand(x=[1, 2, 3])
```

This `expand` function creates three mapped `add` tasks, one for each entry in the `x` input list. The `partial` function specifies a value for `y` that remains constant in each task.

There are a couple of things to keep in mind when working with mapped tasks:

- You *can* use the results of an upstream task as the input to a mapped task (in fact, this is one of the most powerful possibilities of this feature). The upstream task must return a value in a `dict` or `list` form. If you're using traditional operators (ie. not [decorated tasks](https://airflow.apache.org/docs/apache-airflow/2.0.0/concepts.html#python-task-decorator)), the mapping values must be stored in XComs.
- You *can* map over multiple parameters. See the 'Mapping over multiple parameters' section below.
- You *can* use the results of a mapped task as input to a downstream mapped task.
- You *can* have a mapped task that results in no task instances (e.g. if your upstream task that generates the mapping values returns an empty list). In this case, the mapped task will be marked skipped, and downstream tasks will be run according to the trigger rules you set (by default, downstream tasks will also be skipped).
- Some parameters *are not* mappable. For example, `task_id`, `pool`, and many `BaseOperator` arguments are not mappable.
- `expand()` only accepts keyword arguments.

For more high level examples of how to apply dynamic task mapping functions in different cases, check out the [Apache Airflow documentation](https://airflow.apache.org/docs/apache-airflow/2.3.0/concepts/dynamic-task-mapping.html).

The Airflow UI gives us observability for mapped tasks in both the **Graph View** and the **Grid View**.

In the **Graph View**, any mapped tasks will be indicated by a set of brackets `[ ]` following the task ID. The number in the brackets will update for each DAG run to reflect how many mapped instances were created.

![Mapped Graph](https://assets2.astronomer.io/main/guides/dynamic-tasks/mapped_task_graph.png)

Clicking on the mapped task, we have a new **Mapped Instances** drop down where we can select a specific mapped task run to perform actions on.

![Mapped Actions](https://assets2.astronomer.io/main/guides/dynamic-tasks/mapped_instances_task_actions.png)

Selecting one of the mapped instances provides links to other views like you would see for any other Airflow task: Instance Details, Rendered, Log, XCom, etc.

![Mapped Views](https://assets2.astronomer.io/main/guides/dynamic-tasks/mapped_instance_views.png)

Similarly, the **Grid View** shows task details and history for each mapped task. All mapped tasks will be combined into one row on the grid (shown as `load_files_to_snowflake [ ]` in the following example). Clicking into that task will provide details on each individual mapped instance.

![Mapped Grid](https://assets2.astronomer.io/main/guides/dynamic-tasks/mapped_grid_view.png)

## Mapping over the result of another operator

It is possible to use the output of an upstream operator as the input over which a downstream task is being mapped over. A possible use case is shown below in the ETL example.

In this section we will show how to pass mapping information to a downstream task for the following cases:

- Both tasks are defined using the TaskFlowAPI.
- The upstream task is defined using the TaskFlowAPI and the downstream task is using a traditional operator.
- The upstream task is defined using a traditional operator and the downstream task is defined using the TaskFlowAPI.
- Both tasks are defined using traditional operators.

If both tasks are defined using the TaskFlowAPI, you can provide the call of the upstream task to the keyword parameter that is being expanded over.

```Python
@task
def one_two_three_TF():
    return [1,2,3]

@task
def plus_10_TF(x):
    return x+10

plus_10_TF.partial().expand(x=one_two_three_TF())
```

Passing data from an upstream task defined using the TaskFlowAPI to a downstream traditional operator works in a very similar fashion. Note that the format of the mapping information returned by the upstream task may need to be modified to be accepted by the `op_args` argument of the traditional `PythonOperator`.

```Python
@task
def one_two_three_TF():
    # this adjustment is due to op_args expecting each argument as a list
    return [[1],[2],[3]]  

def plus_10_traditional(x):
    return x+10

plus_10_task = PythonOperator.partial(
    task_id="plus_10_task",
    python_callable=plus_10_traditional
).expand(
    op_args=one_two_three_TF()
)
```

If you are mapping over the results of a traditional operator, you need to extract the return value using the `XComArg` object.

```Python
from airflow import XComArg

def one_two_three_traditional():
    return [1,2,3]

@task
def plus_10_TF(x):
    return x+10

one_two_three_task = PythonOperator(
    task_id="one_two_three_task",
    python_callable=one_two_three_traditional
)

plus_10_TF.partial().expand(x=XComArg(one_two_three_task))
```

The `XComArg` object can also be used to map a traditional operator over the results of another traditional operator.

```Python
from airflow import XComArg

def one_two_three_traditional():
    # this adjustment is due to op_args expecting each argument as a list
    return [[1],[2],[3]]

def plus_10_traditional(x):
    return x+10

one_two_three_task = PythonOperator(
    task_id="one_two_three_task",
    python_callable=one_two_three_traditional
)

plus_10_task = PythonOperator.partial(
    task_id="plus_10_task",
    python_callable=plus_10_traditional
).expand(
    op_args=XComArg(one_two_three_task)
)

# when only using traditional operators, define dependencies explicitly
one_two_three_task >> plus_10_task
```

## Mapping over multiple parameters

There are three different ways to map over multiple parameters:

- **Cross-Product**: Mapping over 2 or more *keyword* arguments results in a mapped task instance for each possible combination of inputs. This type of mapping uses the `expand()` function.
- **Sets of keyword arguments**: Mapping over 2 or more sets of one or more *keyword* arguments results in a mapped task instance for every set. This type of mapping uses the `expand_kwargs()` function.
- **Zip**: Mapping over a set of *positional* arguments created with Python's built-in `zip()` function or with the `.zip()` method of an XComArg results in one mapped task for every set of positional arguments. Each set of positional arguments is passed to the same *keyword* argument of the operator. This type of mapping uses the `expand()` function.

### Cross-Product

The default behavior of the `expand()` function is to create a mapped task instance for every possible combination of all provided inputs. For example, if you map over 3 keyword arguments and provide 2 options to the first, 4 options to the second, and 5 options to the third, you would create 2x4x5=40 mapped task instances. One common use case for this method is tuning model hyperparameters.

The task definition below maps over 3 options for  the `bash_command` parameter and 3 options for the `env` parameter. This will result in 3x3=9 mapped task instances. Each bash command runs with each definition for the environment variable `WORD`.

```Python
cross_product_example = BashOperator.partial(
    task_id="cross_product_example"
).expand(
    bash_command=[
        "echo $WORD", # prints the env variable WORD
        "echo `expr length $WORD`", # prints the number of letters in WORD
        "echo ${WORD//e/X}" # replaces each "e" in WORD with "X"
    ],
    env=[
        {"WORD": "hello"},
        {"WORD": "tea"},
        {"WORD": "goodbye"}
    ]
)
```

The nine mapped task instances of the task `cross_product_example` run all possible combinations of the bash command with the env variable:

- Map index 0: `hello`
- Map index 1: `tea`
- Map index 2: `goodbye`
- Map index 3: `5`
- Map index 4: `3`
- Map index 5: `7`
- Map index 6: `hXllo`
- Map index 7: `tXa`
- Map index 8: `goodbyX`

### Sets of keyword arguments

To map over sets of inputs to two or more keyword arguments (kwargs), you can use the `expand_kwargs()` function in Airflow 2.4+. You can provide sets of parameters as a list containing a dictionary or as an `XComArg`. The operator gets 3 sets of commands, resulting in 3 mapped task instances.

```Python
# input sets of kwargs directly as a list[dict] PENDING IMPLEMENTATION!
t1 = BashOperator.partial(task_id="t1").expand_kwargs(
    [
        {"bash_command": "echo $WORD", "env" : {"WORD": "hello"}},
        {"bash_command": "echo `expr length $WORD`", "env" : {"WORD": "tea"}},
        {"bash_command": "echo ${WORD//e/X}", "env" : {"WORD": "goodbye"}}
    ]
)
```

The task `t1` will have 3 mapped task instances printing their results into the logs:

- Map index 0: `hello`
- Map index 1: `3`
- Map index 2: `goodbyX`

### Zip

In Airflow 2.4+ you can provide sets of positional arguments to the same keyword argument, for example to the `op_args` keyword argument of the `PythonOperator`. You can use the built-in Python function [`zip()`](https://docs.python.org/3/library/functions.html#zip) if your inputs are in the form of iterables such as tuples, dictionaries or lists and the `.zip()` method of the `XComArg` object if your inputs are coming from XComs.

> **Note**: It is currently not possible to provide XComArg objects to the built-in Python function `zip()` or to directly provide iterables to the `.zip()` method of the XComArg object.

#### Built-in Python function zip()

The `zip()` function takes in an arbitrary number of iterables (for example lists) and uses their elements to create a zip-object containing tuples. There will be as many tuples as there are elements in the shortest iterable. Each tuple contains one element from every iterable provided. For example:

- `zip(["a", "b", "c"], [1, 2, 3], ["hi", "bye", "tea"])` will result in a zip object containing: `("a", 1, "hi"), ("b", 2, "bye"), ("c", 3, "tea")`
- `zip(["a", "b"], [1], ["hi", "bye"], [19, 23], ["x", "y", "z"])` will result in a zip object containing only one tuple: `("a", 1, "hi", 19, "x")` because the shortest list provided only contains one element.
- It is also possible to zip together different types of iterables: `zip(["a", "b"], {"hi", "bye"}, (19, 23))` will result in a zip object containing: `('a', 'hi', 19), ('b', 'bye', 23)`.

The code snippet below shows how a list of zipped arguments can be provided to the `expand()` function in order to create mapped tasks over sets of positional arguments. Each set of positional arguments is passed to the keyword argument `zipped_x_y_z`.

```Python
# use the zip function to create three-tuples out of three lists
zipped_arguments = list(zip([1,2,3], [10,20,30], [100,200,300]))
# zipped_arguments contains: [(1,10,100), (2,20,200), (3,30,300)]

# creating the mapped task instances using the TaskFlowAPI
@task
def add_numbers(zipped_x_y_z):
    return zipped_x_y_z[0] + zipped_x_y_z[1] + zipped_x_y_z[2]

add_numbers.expand(zipped_x_y_z=zipped_arguments)
```

The task `add_numbers` will have three mapped task instances each performing one calculation with the results being:

- Map index 0: `111`
- Map index 1: `222`
- Map index 2: `333`

#### .zip() method of the XComArg object

It is also possible to zip `XComArg` objects. If the upstream task has been defined using the TaskFlow API, simply provide the function call. If the upstream task used a traditional operator, provide the `XComArg(task_object)`. Below you can see an example of the results of two TaskFlowAPI tasks and one traditional operator being zipped together to form the `zipped_arguments` (`[(1,10,100), (2,1000,200), (1000,1000,300)]`).

To mimic the behavior of the [`zip_longest()`](https://docs.python.org/3/library/itertools.html#itertools.zip_longest) function from the `itertools` package, you can add an optional keyword argument `fillvalue` to the `.zip()` method. If `fillvalue` is specified, the method will produce as many tuples as the longest input has elements, missing elements are filled with the default value. If `fillvalue` would not have been specified in the example below, `zipped_arguments` would only contain one tuple `[(1,10,100)]` since the shortest list provided to the `.zip()` method is only one element long.

```Python
from airflow import XComArg

@task
def one_two_three():
    return [1,2]

@task
def ten_twenty_thirty():
    return [10]

def one_two_three_hundred():
    return [100,200,300]

one_two_three_hundred_task = PythonOperator(
    task_id="one_two_three_hundred_task",
    python_callable=one_two_three_hundred
)

zipped_arguments = one_two_three().zip(
    ten_twenty_thirty(),
    XComArg(one_two_three_hundred_task),
    fillvalue=1000
)
# zipped_arguments contains [(1,10,100), (2,1000,200), (1000,1000,300)]

# creating the mapped task instances using the TaskFlowAPI
@task
def add_nums(zipped_x_y_z):
    print(zipped_x_y_z)
    return zipped_x_y_z[0] + zipped_x_y_z[1] + zipped_x_y_z[2]

add_nums.expand(zipped_x_y_z=zipped_arguments)
```

The add_nums task will have three mapped instances with the following results:

- Map index 0: `111` (1+10+100)
- Map index 1: `1202` (2+1000+200)
- Map index 2: `2300` (1000+1000+300)

## Example implementations

In this section we'll show how dynamic task mapping can be implemented for two classic use cases: ELT and ML Ops. The first implementation will use traditional Airflow operators, and the second will use decorated functions and the TaskFlow API.

### ELT

For our first example, we'll implement one of the most common use cases for dynamic tasks: processing files in S3. In this scenario, we will use an ELT framework to extract data from files in S3, load the data into Snowflake, and transform the data using Snowflake's built-in compute. We assume that files will be dropped daily, but we don't know how many will arrive each day. We'll leverage dynamic task mapping to create a unique task for each file at runtime. This gives us the benefit of atomicity, better observability, and easier recovery from failures.

> **Note:** Code for this example can be found in [this repo](https://github.com/astronomer/dynamic-task-mapping-tutorial).

The DAG below completes the following steps:

1. Use a decorated Python operator to get the current list of files from S3. The S3 prefix passed to this function is parameterized with `ds_nodash` so it pulls files only for the execution date of the DAG run (e.g. for a DAG run on April 12th, we would assume the files landed in a folder named `20220412/`).
2. Using the results of the first task, map an `S3ToSnowflakeOperator` for each file.
3. Move the daily folder of processed files into a `processed/` folder while,
4. Simultaneously (with Step 3), run a Snowflake query that transforms the data. The query is located in a separate SQL file in our `include/` directory.
5. Delete the folder of daily files now that it has been moved to `processed/` for record keeping.

```python
from airflow import DAG
from airflow.decorators import task
from airflow.providers.snowflake.transfers.s3_to_snowflake import (
    S3ToSnowflakeOperator
)
from airflow.providers.snowflake.operators.snowflake import SnowflakeOperator
from airflow.providers.amazon.aws.hooks.s3 import S3Hook
from airflow.providers.amazon.aws.operators.s3_copy_object import (
    S3CopyObjectOperator
)
from airflow.providers.amazon.aws.operators.s3_delete_objects import (
    S3DeleteObjectsOperator
)

from datetime import datetime

@task
def get_s3_files(current_prefix):

    s3_hook = S3Hook(aws_conn_id='s3')

    current_files = s3_hook.list_keys(
        bucket_name='my-bucket',
        prefix=current_prefix+"/",
        start_after_key=current_prefix+"/"
    )

    return [[file] for file in current_files]


with DAG(
    dag_id='mapping_elt',
    start_date=datetime(2022, 4, 2),
    catchup=False,
    template_searchpath='/usr/local/airflow/include',
    schedule_interval='@daily'
) as dag:

    copy_to_snowflake = S3ToSnowflakeOperator.partial(
        task_id='load_files_to_snowflake',
        stage='MY_STAGE',
        table='COMBINED_HOMES',
        schema='MYSCHEMA',
        file_format="(type = 'CSV',field_delimiter = ',', skip_header=1)",
        snowflake_conn_id='snowflake'
    ).expand(
        s3_keys=get_s3_files(current_prefix="{{ ds_nodash }}")
    )

    move_s3 = S3CopyObjectOperator(
        task_id='move_files_to_processed',
        aws_conn_id='s3',
        source_bucket_name='my-bucket',
        source_bucket_key="{{ ds_nodash }}"+"/",
        dest_bucket_name='my-bucket',
        dest_bucket_key="processed/"+"{{ ds_nodash }}"+"/"
    )

    delete_landing_files = S3DeleteObjectsOperator(
        task_id='delete_landing_files',
        aws_conn_id='s3',
        bucket='my-bucket',
        prefix="{{ ds_nodash }}"+"/"
    )

    transform_in_snowflake = SnowflakeOperator(
        task_id='run_transformation_query',
        sql='/transformation_query.sql',
        snowflake_conn_id='snowflake'
    )

    copy_to_snowflake >> [move_s3, transform_in_snowflake]
    move_s3 >> delete_landing_files
```

The Graph View of the DAG looks like this:

![ELT Graph](https://assets2.astronomer.io/main/guides/dynamic-tasks/mapping_elt_graph.png)

When dynamically mapping tasks, make note of the format needed for the parameter you are mapping on. In the example above, we write our own Python function to get the S3 keys because the `S3toSnowflakeOperator` requires *each* `s3_key` parameter to be in a list format, and the `s3_hook.list_keys` function returns a single list with all keys. By writing our own simple function, we can turn the hook results into a list of lists that can be used by the downstream operator.

### ML Ops

 Dynamic tasks can also be very useful for productionizing machine learning pipelines. ML Ops often includes some sort of dynamic component. The following use cases are common:

- **Training different models:** You have a reusable pipeline that you use to create a separate DAG for each data source. For each DAG, you point your generic pipeline to your data source, and to a list of models you want to experiment with in parallel. That list of models might change periodically, but by leveraging dynamic task mapping, you can always have a single task per model without any user intervention when the models change. You also maintain all of the history for any models you have trained in the past, even if they are no longer included in your list.  
- **Hyperparameter training a model:** You have a single model that you want to hyperparameter tune before publishing results from the best set of parameters. With dynamic task mapping, you can grab your parameters from any external system at runtime, giving you full flexibility and history.
- **Creating a different model for each customer:** You have a model that you need to train separately for each individual customer. Your customer list changes frequently, and you need to retain the history of any previous customer models. This can be a tricky use case to implement with dynamic *DAGs*, because the history of any removed DAGs is not retained in the Airflow UI, and performance issues can arise if the customer list is long. With dynamic tasks, you can maintain a single DAG that updates as needed based on the current list of customers at runtime.

 In the example DAG below, we implement the first of these use cases. We also highlight how dynamic task mapping is simple to implement with decorated tasks.

 ```python
from airflow.decorators import task, dag, task_group
from airflow.providers.google.cloud.hooks.bigquery import BigQueryHook

from datetime import datetime

import logging
import mlflow

import pandas as pd

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb

import include.metrics as metrics
from include.grid_configs import models, params


mlflow.set_tracking_uri('http://host.docker.internal:5000')
try:
    # Creating an experiment
    mlflow.create_experiment('census_prediction')
except:
    pass
# Setting the environment with the created experiment
mlflow.set_experiment('census_prediction')

mlflow.sklearn.autolog()
mlflow.lightgbm.autolog()

@dag(
    start_date=datetime(2022, 1, 1),
    schedule_interval=None,
    catchup=False
)
def mlflow_multimodel_example():

    @task
    def load_data():
        """
          Pull Census data from Public BigQuery and save as Pandas dataframe
          in GCS bucket with XCom
        """

        bq = BigQueryHook()
        sql = """
        SELECT * FROM `bigquery-public-data.ml_datasets.census_adult_income`
        """

        return bq.get_pandas_df(sql=sql, dialect='standard')


    @task
    def preprocessing(df: pd.DataFrame):
        """Clean Data and prepare for feature engineering

        Returns pandas dataframe via Xcom to GCS bucket.

        Keyword arguments:
        df -- Raw data pulled from BigQuery to be processed.
        """

        df.dropna(inplace=True)
        df.drop_duplicates(inplace=True)

        # Clean Categorical Variables (strings)
        cols = df.columns
        for col in cols:
            if df.dtypes[col]=='object':
                df[col] =df[col].apply(lambda x: x.rstrip().lstrip())


        # Rename up '?' values as 'Unknown'
        df['workclass'] = df['workclass'].apply(
            lambda x: 'Unknown' if x == '?' else x
        )
        df['occupation'] = df['occupation'].apply(
            lambda x: 'Unknown' if x == '?' else x
        )
        df['native_country'] = df['native_country'].apply(
            lambda x: 'Unknown' if x == '?' else x
        )


        # Drop Extra/Unused Columns
        df.drop(
            columns=['education_num', 'relationship', 'functional_weight'],
            inplace=True
        )

        return df


    @task
    def feature_engineering(df: pd.DataFrame):
        """Feature engineering step

        Returns pandas dataframe via XCom to GCS bucket.

        Keyword arguments:
        df -- data from previous step pulled from BigQuery to be processed.
        """

        # Onehot encoding
        df = pd.get_dummies(df, prefix='workclass', columns=['workclass'])
        df = pd.get_dummies(df, prefix='education', columns=['education'])
        df = pd.get_dummies(df, prefix='occupation', columns=['occupation'])
        df = pd.get_dummies(df, prefix='race', columns=['race'])
        df = pd.get_dummies(df, prefix='sex', columns=['sex'])
        df = pd.get_dummies(
            df, prefix='income_bracket', columns=['income_bracket']
        )
        df = pd.get_dummies(
            df, prefix='native_country', columns=['native_country']
        )

        # Bin Ages
        df['age_bins'] = pd.cut(
            x=df['age'],
            bins=[16,29,39,49,59,100],
            labels=[1, 2, 3, 4, 5]
        )

        # Dependent Variable
        df['never_married'] = df['marital_status'].apply(
            lambda x: 1 if x == 'Never-married' else 0
        )

        # Drop redundant column
        df.drop(
            columns=['income_bracket_<=50K', 'marital_status', 'age'],
            inplace=True
        )

        return df

    @task
    def get_models():
        """
        Returns list of models to train from by reading a file in the
        include/ directory.
        We assume this file has two parameters for each model entry:
        model, and params
        """

        return [models]

    @task()
    def train(
        df: pd.DataFrame,
        model_type=models,
        model=models[model],
        grid_params=models[params],
        **kwargs
    ):
        """Train and validate model using a grid search for the optimal
        parameter values and a five fold cross validation.

        Returns accuracy score via XCom to GCS bucket.

        Keyword arguments:
        df -- data from previous step pulled from BigQuery to be processed.
        """

        y = df['never_married']
        X = df.drop(columns=['never_married'])

        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=55, stratify=y
        )

        grid_search = GridSearchCV(
            model, param_grid=grid_params, verbose=1, cv=5, n_jobs=-1
        )

        with mlflow.start_run(run_name=f'{model_type}_{kwargs["run_id"]}'):

            logging.info('Performing Gridsearch')
            grid_search.fit(X_train, y_train)

            logging.info(f'Best Parameters\n{grid_search.best_params_}')
            best_params = grid_search.best_params_

            if model_type == 'lgbm':

                train_set = lgb.Dataset(X_train, label=y_train)
                test_set = lgb.Dataset(X_test, label=y_test)

                best_params['metric'] = ['auc', 'binary_logloss']

                logging.info('Training model with best parameters')
                clf = lgb.train(
                    train_set=train_set,
                    valid_sets=[train_set, test_set],
                    valid_names=['train', 'validation'],
                    params=best_params,
                    early_stopping_rounds=5
                )

            else:
                logging.info('Training model with best parameters')
                clf = LogisticRegression(
                     penalty=best_params['penalty'],
                     C=best_params['C'],
                     solver=best_params['solver']
                ).fit(X_train, y_train)

            y_pred_class = metrics.test(clf, X_test)

            # Log Classification Report, Confusion Matrix, and ROC Curve
            metrics.log_all_eval_metrics(y_test, y_pred_class)

    df = load_data()
    clean_data = preprocessing(df)
    features = feature_engineering(clean_data)
    train_modes = train.partial(features).expand(get_models())

dag = mlflow_multimodel_example()
```

Note that in this example, our model information that we map on is pulled from a `grid_configs` file in our `include/` directory, which looks like this:

```python
from numpy.random.mtrand import seed
from sklearn.linear_model import LogisticRegression
import lightgbm as lgb


models = {
    'lgbm': {
        'model': lgb.LGBMClassifier(
            objective='binary',
            metric=['auc', 'binary_logloss'],
            seed=55, boosting_type='gbdt'
        ),
        'params': {
            'learning_rate': [0.01, .05, .1],
            'n_estimators': [50, 100, 150],
            'num_leaves': [31, 40, 80],
            'max_depth': [16, 24, 31, 40]
        }
    },
    'log_reg': {
        'model': LogisticRegression(max_iter=500), boosting_type='gbdt'),
        'params': {
            'penalty': ['l1','l2','elasticnet'],
            'C': [0.001, 0.01, 0.1, 1, 10, 100],
            'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']
        }
    }  
}
```

The model information could come from any external system as well, with an update to the `get_models()` task. The only requirement is that the resulting map input is a dict or list.

Also note that for decorated tasks like in the DAG above, the mapping parameters are automatically passed by calling the proper functions, leveraging the TaskFlow API to avoid explicit calling of XCom.
